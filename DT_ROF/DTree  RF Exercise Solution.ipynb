{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Prep\n",
    "\n",
    "I have skipped , intermediate steps in the code like looking at value counts and data types. If you are doing data prep from scratch, you will have to go through all those steps in order to take various data prep decisions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import math\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn import tree\n",
    "import numpy as np\n",
    "from sklearn.cross_validation import KFold\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "data_file='~/Dropbox/March onwards/Python Data Science/Data/loans data.csv'\n",
    "ld=pd.read_csv(data_file)\n",
    "\n",
    "\n",
    "for col in [\"Interest.Rate\",\"Debt.To.Income.Ratio\"]:\n",
    "    ld[col]=ld[col].astype(\"str\")\n",
    "    ld[col]=[x.replace(\"%\",\"\") for x in ld[col]]\n",
    "\n",
    "for col in [\"Amount.Requested\",\"Amount.Funded.By.Investors\",\"Open.CREDIT.Lines\",\n",
    "            \"Revolving.CREDIT.Balance\",\n",
    "           \"Inquiries.in.the.Last.6.Months\",\"Interest.Rate\",\"Debt.To.Income.Ratio\"]:\n",
    "    ld[col]=pd.to_numeric(ld[col],errors=\"coerce\")\n",
    "\n",
    "\n",
    "ld[\"LL_36\"]=np.where(ld['Loan.Length']==\"36 months\",1,0)\n",
    "ld.drop('Loan.Length',axis=1,inplace=True)\n",
    "\n",
    "\n",
    "for i in range(len(ld.index)):\n",
    "    if ld[\"Loan.Purpose\"][i] in [\"car\",\"educational\",\"major_purchase\"]:\n",
    "        ld.loc[i,\"Loan.Purpose\"]=\"cem\"\n",
    "    if ld[\"Loan.Purpose\"][i] in [\"home_improvement\",\"medical\",\"vacation\",\"wedding\"]:\n",
    "        ld.loc[i,\"Loan.Purpose\"]=\"hmvw\"\n",
    "    if ld[\"Loan.Purpose\"][i] in [\"credit_card\",\"house\",\"other\",\"small_business\"]:\n",
    "        ld.loc[i,\"Loan.Purpose\"]=\"chos\"\n",
    "    if ld[\"Loan.Purpose\"][i] in [\"debt_consolidation\",\"moving\"]:\n",
    "        ld.loc[i,\"Loan.Purpose\"]=\"dm\"\n",
    "\n",
    "lp_dummies=pd.get_dummies(ld[\"Loan.Purpose\"],prefix=\"LP\")\n",
    "\n",
    "\n",
    "lp_dummies.drop(\"LP_renewable_energy\",1,inplace=True)\n",
    "\n",
    "\n",
    "ld=pd.concat([ld,lp_dummies],1)\n",
    "ld=ld.drop(\"Loan.Purpose\",1)\n",
    "\n",
    "ld=ld.drop([\"State\"],1)\n",
    "\n",
    "ld[\"ho_mort\"]=np.where(ld[\"Home.Ownership\"]==\"MORTGAGE\",1,0)\n",
    "ld[\"ho_rent\"]=np.where(ld[\"Home.Ownership\"]==\"RENT\",1,0)\n",
    "ld=ld.drop([\"Home.Ownership\"],1)\n",
    "\n",
    "\n",
    "ld['f1'], ld['f2'] = zip(*ld['FICO.Range'].apply(lambda x: x.split('-', 1)))\n",
    "\n",
    "ld[\"fico\"]=0.5*(pd.to_numeric(ld[\"f1\"])+pd.to_numeric(ld[\"f2\"]))\n",
    "\n",
    "ld=ld.drop([\"FICO.Range\",\"f1\",\"f2\"],1)\n",
    "\n",
    "ld[\"Employment.Length\"]=ld[\"Employment.Length\"].astype(\"str\")\n",
    "ld[\"Employment.Length\"]=[x.replace(\"years\",\"\") for x in ld[\"Employment.Length\"]]\n",
    "ld[\"Employment.Length\"]=[x.replace(\"year\",\"\") for x in ld[\"Employment.Length\"]]\n",
    "\n",
    "ld[\"Employment.Length\"]=[x.replace(\"n/a\",\"< 1\") for x in ld[\"Employment.Length\"]]\n",
    "ld[\"Employment.Length\"]=[x.replace(\"10+\",\"10\") for x in ld[\"Employment.Length\"]]\n",
    "ld[\"Employment.Length\"]=[x.replace(\"< 1\",\"0\") for x in ld[\"Employment.Length\"]]\n",
    "ld[\"Employment.Length\"]=pd.to_numeric(ld[\"Employment.Length\"],errors=\"coerce\")\n",
    "\n",
    "ld.dropna(axis=0,inplace=True)\n",
    "\n",
    "ld_train, ld_test = train_test_split(ld, test_size = 0.2,random_state=2)\n",
    "\n",
    "x_train=ld_train.drop([\"Interest.Rate\",\"ID\",\"Amount.Funded.By.Investors\"],1)\n",
    "y_train=ld_train[\"Interest.Rate\"]\n",
    "\n",
    "x_test=ld_test.drop([\"Interest.Rate\",\"ID\",\"Amount.Funded.By.Investors\"],1)\n",
    "y_test=ld_test[\"Interest.Rate\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "We need to reset indices so that we can use the data from kfold cv without index errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x_train.reset_index(drop=True,inplace=True)\n",
    "y_train.reset_index(drop=True,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.grid_search import GridSearchCV\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import time\n",
    "start_time = time.time()\n",
    "\n",
    "param_grid1 = {'max_depth':list(range(20,81,10)),'max_features':list(range(7,16,2)),\n",
    "               \"max_leaf_nodes\":list(range(10,100,5))}\n",
    "grid = GridSearchCV(tree.DecisionTreeRegressor(criterion=\"mse\",random_state=2),param_grid=param_grid1,cv=10)\n",
    "grid.fit(x_train,y_train)\n",
    "\n",
    "\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Line number 1,2 and 10 have nothing to do with run of your algorithm , we are putting it to time the runtime. For me as you can see it took around ~1 minutes. It will vary for you depending upon your RAM, and backgroun processes running when you are executing this on your machine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(grid.best_estimator_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll use this , in our final model as given below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dtree=tree.DecisionTreeRegressor(criterion='mse', max_depth=20, max_features=13,\n",
    "           max_leaf_nodes=40, min_samples_leaf=1, min_samples_split=2,\n",
    "           min_weight_fraction_leaf=0.0, presort=False, random_state=2,\n",
    "           splitter='best')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dtree.fit(x_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "predicted=dtree.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "residual=predicted-y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "rmse_dtree=np.sqrt(np.dot(residual,residual)/len(predicted))\n",
    "\n",
    "rmse_dtree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "linear regression model with l1 penalty gave us rmse on test:1.9985397126873645, we see a slight improvement with decision trees"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Prep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data_file='~/Dropbox/March onwards/Python Data Science/Data/paydayloan_collections.csv'\n",
    "pdl=pd.read_csv(data_file)\n",
    "\n",
    "\n",
    "pdl[\"payment\"]=np.where(pdl[\"payment\"]==\"Success\",1,0)\n",
    "\n",
    "\n",
    "k=pdl.columns\n",
    "\n",
    "\n",
    "for col in k:\n",
    "    if pdl[col].dtype=='object':\n",
    "        temp=pd.get_dummies(pdl[col],drop_first=True,prefix=col)\n",
    "        pdl=pd.concat([pdl,temp],1)\n",
    "        pdl.drop([col],axis=1,inplace=True)\n",
    "\n",
    "\n",
    "pdl.dropna(axis=0,inplace=True)\n",
    "\n",
    "\n",
    "pdl_train, pdl_test = train_test_split(pdl, test_size = 0.2,random_state=2)\n",
    "\n",
    "x_train=pdl_train.drop([\"payment\"],1)\n",
    "y_train=pdl_train[\"payment\"]\n",
    "\n",
    "x_test=pdl_test.drop([\"payment\"],1)\n",
    "y_test=pdl_test[\"payment\"]\n",
    "\n",
    "x_train.reset_index(drop=True,inplace=True)\n",
    "y_train.reset_index(drop=True,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.grid_search import RandomizedSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "params={'n_estimators':[100,200,500,700,1000],\n",
    "       'criterion':['gini','entropy'],\n",
    "       'min_samples_split':[5,6,7,8,9,10],\n",
    "       'bootstrap':[True,False],\n",
    "       'max_depth':[None,5,10,15,20],\n",
    "       'max_features':[5,10,15,20,30,40,50],\n",
    "       'min_samples_leaf':[5,6,7,8,9,10]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "clf=RandomForestClassifier(class_weight=\"balanced\",verbose=1,n_jobs=-1)\n",
    "\n",
    "n_iter_search = 20\n",
    "\n",
    "\n",
    "random_search = RandomizedSearchCV(clf, param_distributions=params,\n",
    "                                   n_iter=n_iter_search)\n",
    "random_search.fit(x_train, y_train)\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(random_search.best_estimator_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "clf=RandomForestClassifier(bootstrap=False, class_weight='balanced',\n",
    "            criterion='entropy', max_depth=20, max_features=30,\n",
    "            max_leaf_nodes=None, min_samples_leaf=8, min_samples_split=10,\n",
    "            min_weight_fraction_leaf=0.0, n_estimators=500, n_jobs=-1,\n",
    "            oob_score=False, random_state=None, verbose=1,\n",
    "            warm_start=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "clf.fit(x_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "predicted=clf.predict(x_test)\n",
    "\n",
    "df_test=pd.DataFrame(list(zip(y_test,predicted)),columns=[\"real\",\"predicted\"])\n",
    "\n",
    "k=pd.crosstab(df_test['real'],df_test[\"predicted\"])\n",
    "print(k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "TP=k.iloc[1,1]\n",
    "TN=k.iloc[0,0]\n",
    "FP=k.iloc[0,1]\n",
    "FN=k.iloc[1,0]\n",
    "P=TP+FN\n",
    "N=TN+FP\n",
    "\n",
    "print('Accuracy is :',(TP+TN)/(P+N))\n",
    "print('Sensitivity is :',TP/P)\n",
    "print('Specificity is :',TN/N)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "importances = clf.feature_importances_\n",
    "importances\n",
    "\n",
    "\n",
    "indices = np.argsort(importances)[::-1]\n",
    "\n",
    "# Print the feature ranking\n",
    "print(\"Feature ranking:\")\n",
    "\n",
    "for f in range(x_train.shape[1]):\n",
    "    print(\"%d. feature %s (%f)\" % (f + 1, list(x_train.columns)[f], importances[indices[f]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### partial dependence plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def part_plot(data,classf,varname):\n",
    "    # we need to create a copy otherwise these changes \n",
    "    # will reflect in the original data as well\n",
    "    d=data.copy()\n",
    "    features=d.columns\n",
    "\n",
    "    for f in features:\n",
    "        if f==varname:pass\n",
    "        else:\n",
    "            d[f]=d[f].mean()\n",
    "\n",
    "    d=d.drop_duplicates()\n",
    "    d['response']=pd.Series(list(zip(*classf.predict_proba(d)))[1])\n",
    "\n",
    "    \n",
    "    print(ggplot(d,aes(x=varname,y='response'))+\\\n",
    "    geom_smooth(se=False,span=0.5)+xlab(varname)+\\\n",
    "    ylab('Response')+\\\n",
    "    ggtitle('Partial Dependence Plot \\n Response Vs '+varname))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from ggplot import *\n",
    "part_plot(x_train,clf,'var3')\n",
    "part_plot(x_train,clf,'var4')\n",
    "part_plot(x_train,clf,'var5')\n",
    "part_plot(x_train,clf,'var6')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import ExtraTreesClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are not going to tune parameters here , and will rather use parameter values from randomforest run. You should ideally do parameter tuning seaparately"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "ext=ExtraTreesClassifier(bootstrap=False, class_weight='balanced',\n",
    "            criterion='entropy', max_depth=20, max_features=30,\n",
    "            max_leaf_nodes=None, min_samples_leaf=8, min_samples_split=10,\n",
    "            min_weight_fraction_leaf=0.0, n_estimators=500, n_jobs=-1,\n",
    "            verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ext.fit(x_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "predicted=ext.predict(x_test)\n",
    "\n",
    "df_test=pd.DataFrame(list(zip(y_test,predicted)),columns=[\"real\",\"predicted\"])\n",
    "\n",
    "k=pd.crosstab(df_test['real'],df_test[\"predicted\"])\n",
    "print(k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "TP=k.iloc[1,1]\n",
    "TN=k.iloc[0,0]\n",
    "FP=k.iloc[0,1]\n",
    "FN=k.iloc[1,0]\n",
    "P=TP+FN\n",
    "N=TN+FP\n",
    "\n",
    "print('Accuracy is :',(TP+TN)/(P+N))\n",
    "print('Sensitivity is :',TP/P)\n",
    "print('Specificity is :',TN/N)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "importances = ext.feature_importances_\n",
    "importances\n",
    "\n",
    "\n",
    "indices = np.argsort(importances)[::-1]\n",
    "\n",
    "# Print the feature ranking\n",
    "print(\"Feature ranking:\")\n",
    "\n",
    "for f in range(x_train.shape[1]):\n",
    "    print(\"%d. feature %s (%f)\" % (f + 1, list(x_train.columns)[f], importances[indices[f]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "part_plot(x_train,ext,'var3')\n",
    "part_plot(x_train,ext,'var4')\n",
    "part_plot(x_train,ext,'var5')\n",
    "part_plot(x_train,ext,'var6')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although there is not much difference in performances , but do notice the difference in importance and slight changes in the pattern which extraclassfier extracts from the data in comparison to randomForest Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Prep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_file='~/Dropbox/March onwards/Python Data Science/Data/emissions.csv'\n",
    "em=pd.read_csv(data_file)\n",
    "\n",
    "k=em.columns\n",
    "for col in k:\n",
    "    if em[col].dtype=='object':\n",
    "        temp=pd.get_dummies(em[col],drop_first=True,prefix=col)\n",
    "        em=pd.concat([em,temp],1)\n",
    "        em.drop([col],axis=1,inplace=True)\n",
    "\n",
    "em.dropna(axis=0,inplace=True)\n",
    "\n",
    "em_train, em_test = train_test_split(em, test_size = 0.2,random_state=2)\n",
    "\n",
    "x_train=em_train.drop([\"ppm\"],1)\n",
    "y_train=em_train[\"ppm\"]\n",
    "\n",
    "x_test=em_test.drop([\"ppm\"],1)\n",
    "y_test=em_test[\"ppm\"]\n",
    "\n",
    "x_train.reset_index(drop=True,inplace=True)\n",
    "y_train.reset_index(drop=True,inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from scipy.stats import randint as sp_randint\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.grid_search import RandomizedSearchCV\n",
    "from sklearn.metrics.scorer import make_scorer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To use your own scoring function, you need to define your own loss function . This needs to take take two inputs as shown below .\n",
    "\n",
    "Here x is the real outcome and y is the predicted outcome.\n",
    "\n",
    "function `make_scorer` converts this to a function which we can pass to GridSearchCV/RandomSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def loss_func(x, y):return((np.abs(x-y)).mean())\n",
    "    \n",
    "my_scorer = make_scorer(loss_func, greater_is_better=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "rg = RandomForestRegressor(n_jobs=-1,verbose=1)\n",
    "\n",
    "        \n",
    "param_dist = {\"n_estimators\":[10,100,500,700],\n",
    "              \"max_depth\": [3,5, None],\n",
    "              \"max_features\": sp_randint(5, 11),\n",
    "              \"min_samples_split\": sp_randint(5, 11),\n",
    "              \"min_samples_leaf\": sp_randint(5, 11),\n",
    "              \"bootstrap\": [True, False]}\n",
    "\n",
    "n_iter_search = 20\n",
    "random_search = RandomizedSearchCV(rg, param_distributions=param_dist,\n",
    "                                   n_iter=n_iter_search,\n",
    "                                   scoring=my_scorer,\n",
    "                                   cv=10,\n",
    "                                   random_state=2)\n",
    "random_search.fit(x_train, y_train)\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(random_search.best_estimator_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#help(RandomForestRegressor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "rg = RandomForestRegressor(bootstrap=True, criterion='mse', max_depth=None,\n",
    "           max_features=9, max_leaf_nodes=None, min_samples_leaf=8,\n",
    "           min_samples_split=8, min_weight_fraction_leaf=0.0,\n",
    "           n_estimators=700, n_jobs=-1, \n",
    "           verbose=1, warm_start=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "rg.fit(x_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "predicted=rg.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "residual=predicted-y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "rmse=np.sqrt(np.dot(residual,residual)/len(predicted))\n",
    "\n",
    "rmse"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
